{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLPClassifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbaGFxXa070K"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rozV3q4E1Hfk"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "siGOjoKPJ8ED",
        "outputId": "9f64f9cf-9c6f-4b2b-9799-c51790715c98"
      },
      "source": [
        "plt.imshow(x_train[0], cmap = plt.cm.binary)\n",
        "plt.show()"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOUElEQVR4nO3dX4xUdZrG8ecF8R+DCkuHtAyRGTQmHY1AStgEg+hk8U+iwI2BGERjxAuQmQTiolzAhRdGd2YyihnTqAE2IxPCSITErIMEY4iJoVC2BZVFTeNA+FOE6Dh6gTLvXvRh0mLXr5qqU3XKfr+fpNPV56nT502Fh1Ndp7t+5u4CMPQNK3oAAK1B2YEgKDsQBGUHgqDsQBAXtfJgY8eO9YkTJ7bykEAovb29OnXqlA2UNVR2M7tT0h8kDZf0krs/nbr/xIkTVS6XGzkkgIRSqVQ1q/tpvJkNl/SCpLskdUlaYGZd9X4/AM3VyM/s0yR96u6fu/sZSX+WNCefsQDkrZGyj5f0t35fH8m2/YCZLTazspmVK5VKA4cD0Iimvxrv7t3uXnL3UkdHR7MPB6CKRsp+VNKEfl//PNsGoA01UvY9kq4zs1+Y2cWS5kvals9YAPJW96U3d//ezJZKelN9l95ecfcDuU0GIFcNXWd39zckvZHTLACaiF+XBYKg7EAQlB0IgrIDQVB2IAjKDgRB2YEgKDsQBGUHgqDsQBCUHQiCsgNBUHYgCMoOBEHZgSAoOxAEZQeCoOxAEJQdCIKyA0FQdiAIyg4EQdmBICg7EARlB4Kg7EAQlB0IgrIDQVB2IIiGVnFF+zt79mwy/+qrr5p6/LVr11bNvv322+S+Bw8eTOYvvPBCMl+xYkXVbNOmTcl9L7300mS+cuXKZL569epkXoSGym5mvZK+lnRW0vfuXspjKAD5y+PMfpu7n8rh+wBoIn5mB4JotOwu6a9mttfMFg90BzNbbGZlMytXKpUGDwegXo2W/RZ3nyrpLklLzGzm+Xdw9253L7l7qaOjo8HDAahXQ2V396PZ55OStkqalsdQAPJXd9nNbKSZjTp3W9JsSfvzGgxAvhp5NX6cpK1mdu77vOru/5PLVEPMF198kczPnDmTzN99991kvnv37qrZl19+mdx3y5YtybxIEyZMSOaPPfZYMt+6dWvVbNSoUcl9b7rppmR+6623JvN2VHfZ3f1zSelHBEDb4NIbEARlB4Kg7EAQlB0IgrIDQfAnrjn44IMPkvntt9+ezJv9Z6btavjw4cn8qaeeSuYjR45M5vfff3/V7Oqrr07uO3r06GR+/fXXJ/N2xJkdCIKyA0FQdiAIyg4EQdmBICg7EARlB4LgOnsOrrnmmmQ+duzYZN7O19mnT5+ezGtdj961a1fV7OKLL07uu3DhwmSOC8OZHQiCsgNBUHYgCMoOBEHZgSAoOxAEZQeC4Dp7DsaMGZPMn3322WS+ffv2ZD5lypRkvmzZsmSeMnny5GT+1ltvJfNaf1O+f3/1pQSee+655L7IF2d2IAjKDgRB2YEgKDsQBGUHgqDsQBCUHQiC6+wtMHfu3GRe633lay0v3NPTUzV76aWXkvuuWLEimde6jl7LDTfcUDXr7u5u6HvjwtQ8s5vZK2Z20sz299s2xsx2mNmh7HP6HQwAFG4wT+PXS7rzvG0rJe109+sk7cy+BtDGapbd3d+RdPq8zXMkbchub5CUfp4KoHD1vkA3zt2PZbePSxpX7Y5mttjMymZWrlQqdR4OQKMafjXe3V2SJ/Judy+5e6mjo6PRwwGoU71lP2FmnZKUfT6Z30gAmqHesm+TtCi7vUjS6/mMA6BZal5nN7NNkmZJGmtmRyStlvS0pM1m9rCkw5Lua+aQQ90VV1zR0P5XXnll3fvWug4/f/78ZD5sGL+X9VNRs+zuvqBK9KucZwHQRPy3DARB2YEgKDsQBGUHgqDsQBD8iesQsGbNmqrZ3r17k/u+/fbbybzWW0nPnj07maN9cGYHgqDsQBCUHQiCsgNBUHYgCMoOBEHZgSC4zj4EpN7ued26dcl9p06dmswfeeSRZH7bbbcl81KpVDVbsmRJcl8zS+a4MJzZgSAoOxAEZQeCoOxAEJQdCIKyA0FQdiAIrrMPcZMmTUrm69evT+YPPfRQMt+4cWPd+TfffJPc94EHHkjmnZ2dyRw/xJkdCIKyA0FQdiAIyg4EQdmBICg7EARlB4LgOntw8+bNS+bXXnttMl++fHkyT73v/BNPPJHc9/Dhw8l81apVyXz8+PHJPJqaZ3Yze8XMTprZ/n7b1pjZUTPbl33c3dwxATRqME/j10u6c4Dtv3f3ydnHG/mOBSBvNcvu7u9IOt2CWQA0USMv0C01s57saf7oancys8VmVjazcqVSaeBwABpRb9n/KGmSpMmSjkn6bbU7unu3u5fcvdTR0VHn4QA0qq6yu/sJdz/r7v+UtE7StHzHApC3uspuZv3/tnCepP3V7gugPdS8zm5mmyTNkjTWzI5IWi1plplNluSSeiU92sQZUaAbb7wxmW/evDmZb9++vWr24IMPJvd98cUXk/mhQ4eS+Y4dO5J5NDXL7u4LBtj8chNmAdBE/LosEARlB4Kg7EAQlB0IgrIDQZi7t+xgpVLJy+Vyy46H9nbJJZck8++++y6ZjxgxIpm/+eabVbNZs2Yl9/2pKpVKKpfLA651zZkdCIKyA0FQdiAIyg4EQdmBICg7EARlB4LgraSR1NPTk8y3bNmSzPfs2VM1q3UdvZaurq5kPnPmzIa+/1DDmR0IgrIDQVB2IAjKDgRB2YEgKDsQBGUHguA6+xB38ODBZP78888n89deey2ZHz9+/IJnGqyLLkr/8+zs7Ezmw4ZxLuuPRwMIgrIDQVB2IAjKDgRB2YEgKDsQBGUHguA6+09ArWvZr776atVs7dq1yX17e3vrGSkXN998czJftWpVMr/33nvzHGfIq3lmN7MJZrbLzD4yswNm9uts+xgz22Fmh7LPo5s/LoB6DeZp/PeSlrt7l6R/l7TEzLokrZS0092vk7Qz+xpAm6pZdnc/5u7vZ7e/lvSxpPGS5kjakN1tg6S5zRoSQOMu6AU6M5soaYqk9ySNc/djWXRc0rgq+yw2s7KZlSuVSgOjAmjEoMtuZj+T9BdJv3H3v/fPvG91yAFXiHT3bncvuXupo6OjoWEB1G9QZTezEeor+p/c/dyfQZ0ws84s75R0sjkjAshDzUtvZmaSXpb0sbv/rl+0TdIiSU9nn19vyoRDwIkTJ5L5gQMHkvnSpUuT+SeffHLBM+Vl+vTpyfzxxx+vms2ZMye5L3+imq/BXGefIWmhpA/NbF+27Un1lXyzmT0s6bCk+5ozIoA81Cy7u++WNODi7pJ+le84AJqF50lAEJQdCIKyA0FQdiAIyg4EwZ+4DtLp06erZo8++mhy33379iXzzz77rK6Z8jBjxoxkvnz58mR+xx13JPPLLrvsgmdCc3BmB4Kg7EAQlB0IgrIDQVB2IAjKDgRB2YEgwlxnf++995L5M888k8z37NlTNTty5EhdM+Xl8ssvr5otW7YsuW+tt2seOXJkXTOh/XBmB4Kg7EAQlB0IgrIDQVB2IAjKDgRB2YEgwlxn37p1a0N5I7q6upL5Pffck8yHDx+ezFesWFE1u+qqq5L7Ig7O7EAQlB0IgrIDQVB2IAjKDgRB2YEgKDsQhLl7+g5mEyRtlDROkkvqdvc/mNkaSY9IqmR3fdLd30h9r1Kp5OVyueGhAQysVCqpXC4PuOryYH6p5ntJy939fTMbJWmvme3Ist+7+3/lNSiA5hnM+uzHJB3Lbn9tZh9LGt/swQDk64J+ZjeziZKmSDr3Hk9LzazHzF4xs9FV9llsZmUzK1cqlYHuAqAFBl12M/uZpL9I+o27/13SHyVNkjRZfWf+3w60n7t3u3vJ3UsdHR05jAygHoMqu5mNUF/R/+Tur0mSu59w97Pu/k9J6yRNa96YABpVs+xmZpJelvSxu/+u3/bOfnebJ2l//uMByMtgXo2fIWmhpA/N7Nzaw09KWmBmk9V3Oa5XUnrdYgCFGsyr8bslDXTdLnlNHUB74TfogCAoOxAEZQeCoOxAEJQdCIKyA0FQdiAIyg4EQdmBICg7EARlB4Kg7EAQlB0IgrIDQdR8K+lcD2ZWkXS436axkk61bIAL066ztetcErPVK8/ZrnH3Ad//raVl/9HBzcruXipsgIR2na1d55KYrV6tmo2n8UAQlB0Iouiydxd8/JR2na1d55KYrV4tma3Qn9kBtE7RZ3YALULZgSAKKbuZ3WlmB83sUzNbWcQM1ZhZr5l9aGb7zKzQ9aWzNfROmtn+ftvGmNkOMzuUfR5wjb2CZltjZkezx26fmd1d0GwTzGyXmX1kZgfM7NfZ9kIfu8RcLXncWv4zu5kNl/R/kv5D0hFJeyQtcPePWjpIFWbWK6nk7oX/AoaZzZT0D0kb3f2GbNszkk67+9PZf5Sj3f0/22S2NZL+UfQy3tlqRZ39lxmXNFfSgyrwsUvMdZ9a8LgVcWafJulTd//c3c9I+rOkOQXM0fbc/R1Jp8/bPEfShuz2BvX9Y2m5KrO1BXc/5u7vZ7e/lnRumfFCH7vEXC1RRNnHS/pbv6+PqL3We3dJfzWzvWa2uOhhBjDO3Y9lt49LGlfkMAOouYx3K523zHjbPHb1LH/eKF6g+7Fb3H2qpLskLcmerrYl7/sZrJ2unQ5qGe9WGWCZ8X8p8rGrd/nzRhVR9qOSJvT7+ufZtrbg7kezzyclbVX7LUV94twKutnnkwXP8y/ttIz3QMuMqw0euyKXPy+i7HskXWdmvzCziyXNl7StgDl+xMxGZi+cyMxGSpqt9luKepukRdntRZJeL3CWH2iXZbyrLTOugh+7wpc/d/eWf0i6W32vyH8maVURM1SZ65eS/jf7OFD0bJI2qe9p3Xfqe23jYUn/JmmnpEOS3pI0po1m+29JH0rqUV+xOgua7Rb1PUXvkbQv+7i76McuMVdLHjd+XRYIghfogCAoOxAEZQeCoOxAEJQdCIKyA0FQdiCI/wfvpjt5Q0mdXQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu2_mjjsKMa7",
        "outputId": "93100278-fe24-4567-e0f8-e8799580463e"
      },
      "source": [
        "print(x_train[0])"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
            " 0.49411765 0.53333336 0.6862745  0.10196079 0.6509804  1.\n",
            " 0.96862745 0.49803922 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.11764706 0.14117648 0.36862746 0.6039216\n",
            " 0.6666667  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            " 0.88235295 0.6745098  0.99215686 0.9490196  0.7647059  0.2509804\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.19215687\n",
            " 0.93333334 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.9843137  0.3647059  0.32156864\n",
            " 0.32156864 0.21960784 0.15294118 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.07058824 0.85882354 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.99215686 0.7764706  0.7137255\n",
            " 0.96862745 0.94509804 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.3137255  0.6117647  0.41960785 0.99215686\n",
            " 0.99215686 0.8039216  0.04313726 0.         0.16862746 0.6039216\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.05490196 0.00392157 0.6039216  0.99215686 0.3529412\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.54509807 0.99215686 0.74509805 0.00784314 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.04313726\n",
            " 0.74509805 0.99215686 0.27450982 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.13725491 0.94509804\n",
            " 0.88235295 0.627451   0.42352942 0.00392157 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.31764707 0.9411765  0.99215686\n",
            " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.1764706  0.7294118  0.99215686 0.99215686\n",
            " 0.5882353  0.10588235 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.0627451  0.3647059  0.9882353  0.99215686 0.73333335\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.9764706  0.99215686 0.9764706  0.2509804  0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.18039216 0.50980395 0.7176471  0.99215686\n",
            " 0.99215686 0.8117647  0.00784314 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.15294118 0.5803922\n",
            " 0.8980392  0.99215686 0.99215686 0.99215686 0.98039216 0.7137255\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.09411765 0.44705883 0.8666667  0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.7882353  0.30588236 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.09019608 0.25882354 0.8352941  0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.7764706  0.31764707 0.00784314\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.07058824 0.67058825\n",
            " 0.85882354 0.99215686 0.99215686 0.99215686 0.99215686 0.7647059\n",
            " 0.3137255  0.03529412 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.21568628 0.6745098  0.8862745  0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.95686275 0.52156866 0.04313726 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.53333336 0.99215686\n",
            " 0.99215686 0.99215686 0.83137256 0.5294118  0.5176471  0.0627451\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPq6Yn602e0b",
        "outputId": "b99f45a4-33d0-4c7f-be7e-68b7e9cafecd"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7ThEQ5D2pKE"
      },
      "source": [
        "\n",
        "\n",
        "1.   train data has 60000 samples\n",
        "2.   test data has 10000 samples\n",
        "3.   all data images are 28 * 28 pixels\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw9FS7JR236B"
      },
      "source": [
        "x_val  = x_train[50000:60000]\n",
        "x_train = x_train[0:50000]\n",
        "y_val  = y_train[50000:60000]\n",
        "y_train = y_train[0:50000]"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAGZAAgU3p01"
      },
      "source": [
        "reshape the image data to put all bits in a 1d array or in other words flatten\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5oIzFGF3mk4",
        "outputId": "accffcd8-439d-4d1c-adc3-ef374615ac6d"
      },
      "source": [
        "x_train = x_train.reshape(50000, 784)\n",
        "x_val = x_val.reshape(10000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 784)\n",
            "(10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1NkiVj_4fIX"
      },
      "source": [
        "Normalize data to reduce variance and improve learning speed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkMHS0HE4LHz"
      },
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_val = x_val.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "gray_scale = 255\n",
        "x_train /= gray_scale\n",
        "x_val /= gray_scale\n",
        "x_test /= gray_scale"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA1samHU4-i-"
      },
      "source": [
        "hot encoding different way to make all data binary\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rlg1kRl25Jvp"
      },
      "source": [
        "num_classes = 10\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t80VTdE5L3D",
        "outputId": "59f41c60-2bcd-4a4d-9616-703a54d486d3"
      },
      "source": [
        "print(y_train)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw0rq-C_5SXQ"
      },
      "source": [
        "x = tf.placeholder(tf.float32, [None, 784])\n",
        "y = tf.placeholder(tf.float32, [None, 10])"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyW4R8ah7mMn"
      },
      "source": [
        "Forward Pass instantiated\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YhgtTJz51Xj"
      },
      "source": [
        "def mlp(x):\n",
        "    # hidden layer1\n",
        "    w1 = tf.Variable(tf.random_uniform([784,256])) #randomize weight\n",
        "    h1 = tf.nn.relu(tf.matmul(x, w1))\n",
        "    # hidden layer2\n",
        "    w2 = tf.Variable(tf.random_uniform([256,128]))\n",
        "    h2 = tf.nn.relu(tf.matmul(h1, w2))\n",
        "    # output layer\n",
        "    w3 = tf.Variable(tf.random_uniform([128,10]))\n",
        "    logits= tf.matmul(h2, w3) \n",
        "    \n",
        "    return logits"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiWPPp2E7t7v"
      },
      "source": [
        "logits = mlp(x)\n",
        "lossFunction = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "    logits=logits, labels=y))"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Asf53DF-7Vt"
      },
      "source": [
        "https://medium.com/arteos-ai/the-differences-between-sigmoid-and-softmax-activation-function-12adee8cf322"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE2d_LV6_xEO"
      },
      "source": [
        "train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(lossFunction)"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJetwurI_6rr",
        "outputId": "2397dfca-05b5-4d4e-9ff2-226b1dd5316b"
      },
      "source": [
        "# initialize\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# train hyperparameters\n",
        "epoch_cnt = 30\n",
        "batch_size = 1000\n",
        "iteration = len(x_train) // batch_size\n",
        "\n",
        "# Start training\n",
        "with tf.Session() as sess:\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "    for epoch in range(epoch_cnt):\n",
        "        avg_loss = 0\n",
        "        start = 0 \n",
        "        end = batch_size\n",
        "        \n",
        "        for i in range(iteration):\n",
        "            _, loss = sess.run([train_op, lossFunction], #_, loss idc about first output give me second one\n",
        "                               feed_dict={x: x_train[start: end], y: y_train[start: end]})\n",
        "            start += batch_size\n",
        "            end += batch_size\n",
        "            # Compute average loss\n",
        "            avg_loss += loss / iteration\n",
        "            \n",
        "        # Validate model\n",
        "        preds = tf.nn.softmax(logits)  # Apply softmax to logits\n",
        "        correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\n",
        "        # Calculate accuracy\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "        cur_val_acc = accuracy.eval({x: x_val, y: y_val})\n",
        "        print(\"epoch: \"+str(epoch)+\", validation accuracy: \" \n",
        "              + str(cur_val_acc) +', loss: '+str(avg_loss))\n",
        "    \n",
        "    # Test model\n",
        "    preds = tf.nn.softmax(logits)  # Apply softmax to logits\n",
        "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1)) #check if data is accurate with prediction\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    print(\"[Test Accuracy] :\", accuracy.eval({x: x_test, y: y_test}))"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, validation accuracy: 0.1832, loss: 9277.953607177733\n",
            "epoch: 1, validation accuracy: 0.7827, loss: 274.93229591369635\n",
            "epoch: 2, validation accuracy: 0.8661, loss: 16.4748141670227\n",
            "epoch: 3, validation accuracy: 0.8819, loss: 10.391466207504267\n",
            "epoch: 4, validation accuracy: 0.8851, loss: 7.916505851745606\n",
            "epoch: 5, validation accuracy: 0.8915, loss: 6.414077224731445\n",
            "epoch: 6, validation accuracy: 0.8923, loss: 5.4699411725997935\n",
            "epoch: 7, validation accuracy: 0.8859, loss: 5.400930657386779\n",
            "epoch: 8, validation accuracy: 0.899, loss: 4.781930785179139\n",
            "epoch: 9, validation accuracy: 0.9059, loss: 4.052775111198423\n",
            "epoch: 10, validation accuracy: 0.9036, loss: 3.5383604049682624\n",
            "epoch: 11, validation accuracy: 0.9004, loss: 3.130056934356689\n",
            "epoch: 12, validation accuracy: 0.8876, loss: 5.4171368122100825\n",
            "epoch: 13, validation accuracy: 0.8988, loss: 3.785394225120544\n",
            "epoch: 14, validation accuracy: 0.9041, loss: 3.9673352241516113\n",
            "epoch: 15, validation accuracy: 0.9047, loss: 5.08601369857788\n",
            "epoch: 16, validation accuracy: 0.9029, loss: 3.5594302320480344\n",
            "epoch: 17, validation accuracy: 0.9068, loss: 2.9652589941024776\n",
            "epoch: 18, validation accuracy: 0.9049, loss: 2.8884752631187456\n",
            "epoch: 19, validation accuracy: 0.9028, loss: 3.2094847726821896\n",
            "epoch: 20, validation accuracy: 0.9, loss: 3.3439215874671935\n",
            "epoch: 21, validation accuracy: 0.8427, loss: 8.595557110309603\n",
            "epoch: 22, validation accuracy: 0.9101, loss: 4.743452005386353\n",
            "epoch: 23, validation accuracy: 0.9058, loss: 3.2062099170684815\n",
            "epoch: 24, validation accuracy: 0.8988, loss: 2.5837549901008607\n",
            "epoch: 25, validation accuracy: 0.9098, loss: 2.456456179618836\n",
            "epoch: 26, validation accuracy: 0.9152, loss: 2.2263084650039673\n",
            "epoch: 27, validation accuracy: 0.9257, loss: 2.074021406173706\n",
            "epoch: 28, validation accuracy: 0.9109, loss: 2.2619028663635254\n",
            "epoch: 29, validation accuracy: 0.9167, loss: 1.8691291761398312\n",
            "[Test Accuracy] : 0.9147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOeSaeLID9yV"
      },
      "source": [
        "Videos I followed▶\n",
        "1.  https://youtu.be/wQ8BIBpya2k\n",
        "2.  https://youtu.be/tPYj3fFJGjk"
      ]
    }
  ]
}